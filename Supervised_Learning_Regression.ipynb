{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0091b6b1",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h1> Supervised_Learning_Regression\n",
    "        </h1>\n",
    "        <p> <b> Min Khant Soe</b> </p>\n",
    "        <p> ID: 122277 </p>\n",
    "        <p> ^_^ </p>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b956444",
   "metadata": {},
   "source": [
    "<html>\n",
    "        <p> 4. Put in CLASS \n",
    "    </p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c782c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised_Learning_Regression:\n",
    "    \n",
    "    \n",
    "    # p_loss for previous_loss\n",
    "    # c_loss for current_loss\n",
    "    # tol for tolerance\n",
    "    def __init__(self, method = 'batch', p_loss = 10000, alpha = 0.0001, tol = 0.0001, max_iter = 10000000, iter_stop = 0):\n",
    "        \n",
    "        self.method = method\n",
    "        self.p_loss = p_loss\n",
    "        self.alpha = alpha \n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.iter_stop = iter_stop\n",
    "        \n",
    "    def choose_method(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        X_train = self.X\n",
    "        y_train = self.y\n",
    "        \n",
    "        #______________________________________________________________________________________\n",
    "        if self.method == 'Closed_Form':\n",
    "            \n",
    "            from numpy.linalg import inv\n",
    "            theta = inv(X.T @ X) @ X.T @ y\n",
    "            yhat = X_train @ theta\n",
    "            assert y_train.shape == yhat.shape\n",
    "            mse = ((y_train - yhat)**2).sum() / X_train.shape[0]\n",
    "        #________________________________________________________________________________________\n",
    "        elif self.method == 'Batch':\n",
    "            \n",
    "            from time import time\n",
    "            assert X_train.shape[0] == y_train.shape[0]\n",
    "            \n",
    "            self.theta = np.zeros(X_train.shape[1])\n",
    "            \n",
    "            start = time()\n",
    "            # define your for loop\n",
    "            for i in range(self.max_iter):\n",
    "\n",
    "                yhat = self.h_theta(X_train)\n",
    "\n",
    "                error = yhat - y_train\n",
    "\n",
    "                grad = self.gradient(X_train, error)\n",
    "\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                c_loss = self.mse(yhat,y_train)\n",
    "                \n",
    "                difference = np.abs(c_loss - self.p_loss)\n",
    "                if difference < self.tol:\n",
    "                    break\n",
    "                self.p_loss = c_loss\n",
    "            time_taken = time() - start\n",
    "        #_______________________________________________________________________________\n",
    "        elif self.method == 'Stochastic':\n",
    "            \n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "            X_train = self.X\n",
    "            y_train = self.y\n",
    "            \n",
    "            from time import time\n",
    "            \n",
    "            assert X_train.shape[0] == y_train.shape[0]\n",
    "            self.theta = np.zeros(X_train.shape[1])\n",
    "        \n",
    "            start = time()\n",
    "            # define your for loop\n",
    "            for i in range(self.max_iter):\n",
    "\n",
    "                a = np.random.choice(X_train.shape[0])\n",
    "                X_train_a = X_train[a,:]\n",
    "                X_train_a = X_train_a.reshape((1, X_train_a.shape[0]))\n",
    "\n",
    "                y_train_a = y_train[a]\n",
    "\n",
    "                yhat = self.h_theta(X_train_a)\n",
    "\n",
    "                error = yhat - y_train_a\n",
    "\n",
    "                grad = self.gradient(X_train_a, error)\n",
    "\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                c_loss = self.mse(yhat,y_train_a)\n",
    "                difference = np.abs(c_loss - self.p_loss)\n",
    "                if difference < self.tol:\n",
    "                    print(\"Stop the loop\")\n",
    "                    break\n",
    "                p_loss = c_loss\n",
    "\n",
    "            time_taken = time() - start\n",
    "        #___________________________________________________________________________\n",
    "        elif self.method == 'Mini_Batch':\n",
    "            \n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "            X_train = self.X\n",
    "            y_train = self.y\n",
    "            \n",
    "            from time import time\n",
    "            \n",
    "            assert X_train.shape[0] == y_train.shape[0]\n",
    "            self.theta = np.zeros(X_train.shape[1])\n",
    "            \n",
    "            batch_size = 20\n",
    "            start = time()\n",
    "\n",
    "            # define your for loop\n",
    "            for i in range(self.max_iter):\n",
    "\n",
    "                for b in range(X_train.shape[0]):\n",
    "                    X_train_b = X_train[b:b+batch_size,:]\n",
    "                    y_train_b = y_train[b:b+batch_size]\n",
    "                    b = b + batch_size\n",
    "                yhat = self.h_theta(X_train_b)\n",
    "                \n",
    "                error = yhat - y_train_b\n",
    "\n",
    "                grad = self.gradient(X_train_b, error)\n",
    "\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                c_loss = self.mse(yhat,y_train_b)\n",
    "                difference = np.abs(c_loss - self.p_loss)\n",
    "                if difference < self.tol:\n",
    "                    print(\"Stop the loop\")\n",
    "                    break\n",
    "                self.p_loss = c_loss\n",
    "\n",
    "            time_taken = time() - start    \n",
    "            #___________________________________________________________________________\n",
    "    \n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0c92d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (506, 13), y.shape = (506,)\n",
      "X_train.shape = (303, 14), X_test.shape = (203, 14), y_train.shape = (303,), y_test.shape = (203,) \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Preparing data _______________________________________________________________\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "print('X.shape = {}, y.shape = {}'.format(X.shape, y.shape))\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "y = boston.target\n",
    "\n",
    "assert m == y.shape[0] \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# train_test_split _____________________________________________________________\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "\n",
    "# Adding intercept _____________________________________________________________\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "print(\"X_train.shape = {}, X_test.shape = {}, y_train.shape = {}, y_test.shape = {} \".format(X_train.shape, X_test.shape,y_train.shape,y_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3218896e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop the loop\n"
     ]
    }
   ],
   "source": [
    "Model = Supervised_Learning_Regression('Mini_Batch', 10000, 0.0001, 0.0001, 10000000, 0)\n",
    "Model.choose_method(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94636b8",
   "metadata": {},
   "source": [
    "<html>\n",
    "        <p> 1. Batch Gradient Descent (Adding Early Stopping)\n",
    "    </p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9cd61399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop the loop\n",
      "17.502157788267567\n",
      "(303, 14) (303,) (303,)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# X_train, X_test have intercepts that are being concatenated to the data\n",
    "# [1, features\n",
    "#  1, features....]\n",
    "\n",
    "# making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# initialize our w\n",
    "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "# intercept\n",
    "# w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# define the learning rate\n",
    "# later on, you gonna know that it should be better to make it slowly decreasing\n",
    "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "# define our max_iter\n",
    "# typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 1000\n",
    "\n",
    "loss_old = 10000\n",
    "p_loss = loss_old\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "start = time()\n",
    "\n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # 1. yhat = X @ w\n",
    "    # prediction\n",
    "    # yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = h_theta(X_train, theta)\n",
    "\n",
    "    # 2. error = yhat - y_train\n",
    "    # error for use to calculate gradients\n",
    "    # error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "#     print(yhat.shape, error.shape,X_train.shape)\n",
    "    # 3. grad = X.T @ error\n",
    "    # grad (n, ) = (n, m) @ (m, )\n",
    "    # grad for each feature j\n",
    "    grad = gradient(X_train, error)\n",
    "\n",
    "    # 4. w = w - alpha * grad\n",
    "    # update w\n",
    "    # w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "#     print(theta.shape, yhat.shape, grad.shape, error.shape)\n",
    "    \n",
    "    c_loss = mse(yhat,y_train)\n",
    "    difference = np.abs(c_loss-p_loss)\n",
    "    if difference < tol:\n",
    "        print(\"Stop the loop\")\n",
    "        break\n",
    "    p_loss = c_loss\n",
    "    \n",
    "time_taken = time() - start\n",
    "\n",
    "print(mse(yhat, y_train))\n",
    "print(X_train.shape, y_train.shape, error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a4444",
   "metadata": {},
   "source": [
    "<html>\n",
    "        <p> 2. Stochastic Gradient Descent \n",
    "    </p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9949878e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14) (203, 14) (303,)\n",
      "Stop the loop\n",
      "0.0018135894098901778\n",
      "(1, 14) () (1,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# X_train, X_test have intercepts that are being concatenated to the data\n",
    "# [1, features\n",
    "#  1, features....]\n",
    "\n",
    "# making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# initialize our w\n",
    "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "# intercept\n",
    "# w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# define the learning rate\n",
    "# later on, you gonna know that it should be better to make it slowly decreasing\n",
    "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "# define our max_iter\n",
    "# typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 100000000\n",
    "\n",
    "loss_old = 10000\n",
    "p_loss = loss_old\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "start = time()\n",
    "    \n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    a = np.random.choice(X_train.shape[0])\n",
    "    X_train_a = X_train[a,:]\n",
    "    X_train_a = X_train_a.reshape((1, X_train_a.shape[0]))\n",
    "    \n",
    "    y_train_a = y_train[a]\n",
    "\n",
    "    yhat = h_theta(X_train_a, theta)\n",
    "\n",
    "    error = yhat - y_train_a\n",
    "\n",
    "    grad = gradient(X_train_a, error)\n",
    "\n",
    "    theta = theta - alpha * grad\n",
    "    \n",
    "    #early stopping\n",
    "    c_loss = mse(yhat,y_train_a)\n",
    "    difference = np.abs(c_loss-p_loss)\n",
    "    if difference < tol:\n",
    "        print(\"Stop the loop\")\n",
    "        break\n",
    "    p_loss = c_loss\n",
    "\n",
    "time_taken = time() - start\n",
    "\n",
    "print(mse(yhat, y_train_a))\n",
    "print(X_train_a.shape, y_train_a.shape, error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe049a8",
   "metadata": {},
   "source": [
    "<html>\n",
    "        <p> 3. Mini-Batch Gradient Descent\n",
    "    </p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ad73cb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14) (203, 14) (303,)\n",
      "Stop the loop\n",
      "0.02820194158657362\n",
      "(1, 14) (1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# X_train, X_test have intercepts that are being concatenated to the data\n",
    "# [1, features\n",
    "#  1, features....]\n",
    "\n",
    "# making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# initialize our w\n",
    "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "# intercept\n",
    "# w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# define the learning rate\n",
    "# later on, you gonna know that it should be better to make it slowly decreasing\n",
    "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "# define our max_iter\n",
    "# typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 10000000\n",
    "\n",
    "loss_old = 10000\n",
    "p_loss = loss_old\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "batch_size = 20\n",
    "start = time()\n",
    "    \n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    for b in range(X_train.shape[0]):\n",
    "        X_train_b = X_train[b:b+batch_size,:]\n",
    "        y_train_b = y_train[b:b+batch_size]\n",
    "        b = b + batch_size\n",
    "        \n",
    "#     # 1. yhat = X @ w\n",
    "#     # prediction\n",
    "#     # yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = h_theta(X_train_b, theta)\n",
    "    \n",
    "#     # 2. error = yhat - y_train\n",
    "#     # error for use to calculate gradients\n",
    "#     # error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train_b\n",
    "#     print(X_train_a.shape, error.shape)\n",
    "\n",
    "#     # 3. grad = X.T @ error\n",
    "#     # grad (n, ) = (n, m) @ (m, )\n",
    "#     # grad for each feature j\n",
    "    grad = gradient(X_train_b, error)\n",
    "\n",
    "#     # 4. w = w - alpha * grad\n",
    "#     # update w\n",
    "#     # w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "# #     print(theta.shape, yhat.shape, grad.shape, error.shape)\n",
    "    \n",
    "    c_loss = mse(yhat,y_train_b)\n",
    "    difference = np.abs(c_loss-p_loss)\n",
    "    if difference < tol:\n",
    "        print(\"Stop the loop\")\n",
    "        break\n",
    "    p_loss = c_loss\n",
    "\n",
    "time_taken = time() - start\n",
    "\n",
    "print(mse(yhat, y_train_b))\n",
    "print(X_train_b.shape, y_train_b.shape, error.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
