{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fd3313",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h1>  AdaBoost \n",
    "        </h1>\n",
    "        <p> <b> Min Khant Soe</b> </p>\n",
    "        <p> ID: 122277 </p>\n",
    "        <p> ^_^ </p>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebf4d1",
   "metadata": {},
   "source": [
    "Modify the AdaBoost scratch code in our lecture such that:\n",
    "\n",
    "Notice that if err = 0, then  will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "\n",
    "Notice that sklearn version of AdaBoost has a parameter learning_rate. This is in fact the  in front of the  calculation.\n",
    "Attempt to change this  into a parameter called eta, and try different values of it and see whether accuracy is improved. Note that sklearn default this value to 1.\n",
    "\n",
    "Observe that we are actually using sklearn DecisionTreeClassifier. If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above. Attempt to write your own class of class Stump that actually uses weighted errors, instead of weighted gini index. \n",
    "\n",
    "To check whether your stump really works, it should give you still relatively the same accuracy. In addition, if you do not change y to -1, it will result in very bad accuracy. Unlike sklearn version of DecisionTree, it will STILL work even y is not change to -1 since it uses gini index\n",
    "\n",
    "Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e28c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=10)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7767fc9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20) (500,) \n",
      " (300, 20) (200, 20) (300,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape,\"\\n\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2e4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump():\n",
    "    def __init__(self):\n",
    "        # Determines whether threshold should be evaluated as < or >\n",
    "        self.polarity = 1\n",
    "        self.n_index = None\n",
    "        self.threshold = None\n",
    "        # Voting power of the stump\n",
    "        self.alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095a8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ada_Boost():\n",
    "    def __init__(self, S=5, eta=0.5):\n",
    "        self.S = S\n",
    "        self.eta = eta\n",
    "        \n",
    "    def fit(self, X, y): #<----X_train, y_train\n",
    "        m, n = X.shape\n",
    "        \n",
    "        #initial weight\n",
    "        W = np.full(m, 1/m)\n",
    "                \n",
    "        self.clfs = []   #Classifier\n",
    "        \n",
    "        for _ in range(self.S):\n",
    "            clf = Stump()\n",
    "            \n",
    "            #set initial minimum error to infinity\n",
    "            min_err = np.inf\n",
    "\n",
    "            for n in range(n):\n",
    "                n_val = np.sort(np.unique(X[:, n]))\n",
    "                thresholds = (n_val[:-1] + n_val[1:])/2\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    for polarity in [1, -1]:\n",
    "                        y_hat = np.ones(len(y)) #set all to 1\n",
    "                        y_hat[polarity * X[:, n] < polarity * threshold] = -1  #polarity = 1 rule\n",
    "                        err = W[(y_hat != y)].sum()\n",
    "                                        \n",
    "                        #To save the best stump\n",
    "                        if err < min_err:\n",
    "                            clf.polarity = polarity\n",
    "                            clf.threshold = threshold\n",
    "                            clf.n_index = n\n",
    "                            min_err = err\n",
    "        \n",
    "            #After the best stump is found\n",
    "            #Calculate alpha, and reweight samples\n",
    "            eps = 0.000001             #to prevent division by zero\n",
    "            clf.alpha = self.eta * (np.log ((1 - min_err) / (min_err + eps)))            \n",
    "            W = W * np.exp(-clf.alpha * y * y_hat)\n",
    "            W = W / sum (W)\n",
    "\n",
    "            #save clf\n",
    "            self.clfs.append(clf)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        m, n = X.shape\n",
    "        y_hat = np.zeros(m)\n",
    "        for clf in self.clfs:\n",
    "            pred = np.ones(m) #set all to 1\n",
    "            pred[clf.polarity * X[:, clf.n_index] < clf.polarity * clf.threshold] = -1  #polarity = 1 rule\n",
    "            y_hat += clf.alpha * pred\n",
    "\n",
    "        return np.sign(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2797596b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.99      0.66        96\n",
      "           1       0.83      0.05      0.09       104\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.66      0.52      0.37       200\n",
      "weighted avg       0.67      0.50      0.36       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S=20, eta = 1/2)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51037e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      1.00      0.66        96\n",
      "           1       1.00      0.03      0.06       104\n",
      "\n",
      "    accuracy                           0.49       200\n",
      "   macro avg       0.74      0.51      0.36       200\n",
      "weighted avg       0.75      0.49      0.34       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S=20, eta = 1)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d3151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.99      0.66        96\n",
      "           1       0.83      0.05      0.09       104\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.66      0.52      0.37       200\n",
      "weighted avg       0.67      0.50      0.36       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S=20, eta = 1/4)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb0e6bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.99      0.66        96\n",
      "           1       0.86      0.06      0.11       104\n",
      "\n",
      "    accuracy                           0.51       200\n",
      "   macro avg       0.67      0.52      0.38       200\n",
      "weighted avg       0.68      0.51      0.37       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S = 15, eta = 1/2)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "287ce990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.98      0.82        96\n",
      "           1       0.97      0.63      0.77       104\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.84      0.81      0.80       200\n",
      "weighted avg       0.85      0.80      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S = 5, eta = 1/2)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be753d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.95      0.91        96\n",
      "           1       0.95      0.87      0.90       104\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.90       200\n",
      "weighted avg       0.91      0.91      0.90       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ada_Boost(S = 5, eta = 1/10000000)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e818870",
   "metadata": {},
   "source": [
    "Notice that sklearn version of AdaBoost has a parameter learning_rate. This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation. Attempt to change this $\\frac{1}{2}$ into a parameter called eta, and try different values of it and see whether accuracy is improved. Note that sklearn default this value to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7982754",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "Accuracy can be improved by decreasing eta with same S or decreasing S value with same eta.\n",
    "\n",
    "The maximum accuracy I could find from this is 91%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
